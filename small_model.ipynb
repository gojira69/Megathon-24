{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load training and validation datasets\n",
        "train_data = pd.read_csv('twitter/twitter_training.csv')\n",
        "val_data = pd.read_csv('twitter/twitter_validation.csv')\n",
        "\n",
        "# Check the first few rows to understand the data structure\n",
        "print(\"Training Data:\\n\", train_data.head())\n",
        "print(\"Validation Data:\\n\", val_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kS410nQjyus",
        "outputId": "d4ed5caf-d2eb-4610-e74b-990d5c6a1eab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:\n",
            "    2401  Borderlands  Positive  \\\n",
            "0  2401  Borderlands  Positive   \n",
            "1  2401  Borderlands  Positive   \n",
            "2  2401  Borderlands  Positive   \n",
            "3  2401  Borderlands  Positive   \n",
            "4  2401  Borderlands  Positive   \n",
            "\n",
            "  im getting on borderlands and i will murder you all ,  \n",
            "0  I am coming to the borders and I will kill you...     \n",
            "1  im getting on borderlands and i will kill you ...     \n",
            "2  im coming on borderlands and i will murder you...     \n",
            "3  im getting on borderlands 2 and i will murder ...     \n",
            "4  im getting into borderlands and i can murder y...     \n",
            "Validation Data:\n",
            "    3364   Facebook Irrelevant  \\\n",
            "0   352     Amazon    Neutral   \n",
            "1  8312  Microsoft   Negative   \n",
            "2  4371      CS-GO   Negative   \n",
            "3  4433     Google    Neutral   \n",
            "4  6273       FIFA   Negative   \n",
            "\n",
            "  I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâ€™s great auntie as â€˜Hayley canâ€™t get out of bedâ€™ and told to his grandma, who now thinks Iâ€™m a lazy, terrible person ðŸ¤£  \n",
            "0  BBC News - Amazon boss Jeff Bezos rejects clai...                                                                                                                                                                                                  \n",
            "1  @Microsoft Why do I pay for WORD when it funct...                                                                                                                                                                                                  \n",
            "2  CSGO matchmaking is so full of closet hacking,...                                                                                                                                                                                                  \n",
            "3  Now the President is slapping Americans in the...                                                                                                                                                                                                  \n",
            "4  Hi @EAHelp Iâ€™ve had Madeleine McCann in my cel...                                                                                                                                                                                                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data with specified column names\n",
        "train_data = pd.read_csv('twitter/twitter_training.csv', names=['id', 'source', 'sentiment', 'text'])\n",
        "val_data = pd.read_csv('twitter/twitter_validation.csv', names=['id', 'source', 'sentiment', 'text'])\n",
        "\n",
        "# Check the first few rows to confirm\n",
        "print(\"Training Data Sample:\\n\", train_data.head())\n",
        "print(\"Validation Data Sample:\\n\", val_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilVy_8sqDjkT",
        "outputId": "3fcbcc2a-8157-4eda-99fc-33bec29adc71"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Sample:\n",
            "      id       source sentiment  \\\n",
            "0  2401  Borderlands  Positive   \n",
            "1  2401  Borderlands  Positive   \n",
            "2  2401  Borderlands  Positive   \n",
            "3  2401  Borderlands  Positive   \n",
            "4  2401  Borderlands  Positive   \n",
            "\n",
            "                                                text  \n",
            "0  im getting on borderlands and i will murder yo...  \n",
            "1  I am coming to the borders and I will kill you...  \n",
            "2  im getting on borderlands and i will kill you ...  \n",
            "3  im coming on borderlands and i will murder you...  \n",
            "4  im getting on borderlands 2 and i will murder ...  \n",
            "Validation Data Sample:\n",
            "      id     source   sentiment  \\\n",
            "0  3364   Facebook  Irrelevant   \n",
            "1   352     Amazon     Neutral   \n",
            "2  8312  Microsoft    Negative   \n",
            "3  4371      CS-GO    Negative   \n",
            "4  4433     Google     Neutral   \n",
            "\n",
            "                                                text  \n",
            "0  I mentioned on Facebook that I was struggling ...  \n",
            "1  BBC News - Amazon boss Jeff Bezos rejects clai...  \n",
            "2  @Microsoft Why do I pay for WORD when it funct...  \n",
            "3  CSGO matchmaking is so full of closet hacking,...  \n",
            "4  Now the President is slapping Americans in the...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize necessary NLTK data and tools\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"  # Return an empty string for non-string entries\n",
        "    # Remove URLs, mentions, hashtags\n",
        "    text = re.sub(r\"(http\\S+|@\\S+|#\\S+)\", \"\", text)\n",
        "    # Remove numbers and special characters\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Remove stopwords and stem words\n",
        "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "    # Rejoin tokens into the preprocessed sentence\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing to the 'text' column in training and validation data\n",
        "train_data['clean_text'] = train_data['text'].apply(preprocess_text)\n",
        "val_data['clean_text'] = val_data['text'].apply(preprocess_text)\n",
        "\n",
        "# Apply preprocessing to training and validation data\n",
        "train_data['clean_text'] = train_data['text'].apply(preprocess_text)\n",
        "val_data['clean_text'] = val_data['text'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ij8h_qN2kRW-",
        "outputId": "306ca244-d9e7-43f3-858c-2f624b59ff5c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize and fit TF-IDF vectorizer on training data\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_train = tfidf.fit_transform(train_data['clean_text']).toarray()\n",
        "\n",
        "# Transform validation data using the fitted TF-IDF vectorizer\n",
        "X_val = tfidf.transform(val_data['clean_text']).toarray()\n"
      ],
      "metadata": {
        "id": "zvRkOg52lVlW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize label encoder and fit on training labels\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(train_data['sentiment'])\n",
        "y_val = le.transform(val_data['sentiment'])\n"
      ],
      "metadata": {
        "id": "yfl7qOoJlZom"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Initialize and train the model with max_iter set to 5000 for thorough convergence\n",
        "model = LogisticRegression(max_iter=2000)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Prediction with chunking and progress bar\n",
        "chunk_size = 1000  # Customize this chunk size as needed\n",
        "y_val_pred = []\n",
        "\n",
        "for i in tqdm(range(0, len(X_val), chunk_size), desc=\"Validation Prediction Progress\"):\n",
        "    end = min(i + chunk_size, len(X_val))\n",
        "    y_val_pred.extend(model.predict(X_val[i:end]))\n",
        "\n",
        "y_val_pred = np.array(y_val_pred)\n",
        "\n",
        "# Evaluate the model's performance on the validation set\n",
        "report = classification_report(y_val, y_val_pred, target_names=le.classes_)\n",
        "\n",
        "# Display the classification report\n",
        "print(\"### Classification Report Overview ###\")\n",
        "print(report)\n",
        "\n",
        "# Detailed explanation for each row and column:\n",
        "# Class (Row): Each row represents a specific sentiment class:\n",
        "# - Irrelevant: Instances classified as irrelevant.\n",
        "# - Negative: Instances with a negative sentiment.\n",
        "# - Neutral: Instances with a neutral sentiment.\n",
        "# - Positive: Instances with a positive sentiment.\n",
        "\n",
        "# Column descriptions:\n",
        "# - Precision: Percentage of correctly predicted instances for each class out of all predicted as that class.\n",
        "# - Recall: Percentage of correctly predicted instances for each class out of all actual instances of that class.\n",
        "# - F1-Score: The harmonic mean of precision and recall, balancing these two metrics.\n",
        "# - Support: The number of actual samples for each class in the dataset.\n",
        "\n",
        "# Summary Rows:\n",
        "# - Accuracy: The overall accuracy of the model across all classes.\n",
        "# - Macro Avg: The unweighted average of precision, recall, and F1-score across all classes, treating each class equally.\n",
        "# - Weighted Avg: The average of precision, recall, and F1-score, weighted by support (class frequency), which is useful in imbalanced datasets.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSb7wxv9lh0Z",
        "outputId": "31952d3b-5d32-4e8c-a1d5-e0a454dfaa26"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Prediction Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 44.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Classification Report Overview ###\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Irrelevant       0.75      0.70      0.72       172\n",
            "    Negative       0.74      0.85      0.79       266\n",
            "     Neutral       0.86      0.73      0.79       285\n",
            "    Positive       0.82      0.87      0.84       277\n",
            "\n",
            "    accuracy                           0.79      1000\n",
            "   macro avg       0.79      0.79      0.79      1000\n",
            "weighted avg       0.80      0.79      0.79      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}